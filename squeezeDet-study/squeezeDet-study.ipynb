{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is my note of study SqueezeDet "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## squeezeDet is requirements easydict 1.6, it need to download from  https://pypi.org/project/easydict/1.6/\n",
    "## then pip install xxx.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import cv2\n",
    "from datetime import datetime\n",
    "import os.path\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import xrange\n",
    "import tensorflow as tf\n",
    "import threading\n",
    "\n",
    "from config import *\n",
    "from dataset import pascal_voc, kitti\n",
    "from utils.util import sparse_to_dense, bgr_to_rgb, bbox_transform\n",
    "from nets import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine if train_dir is exist, if it is exist will delete it.\n",
    "## Then make a file for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(argv=None): \n",
    "    if tf.gfile.Exists(FLAGS.train_dir): #\n",
    "        tf.gfile.DeleteRecursively(FLAGS.train_dir)\n",
    "\n",
    "    tf.gfile.MakeDirs(FLAGS.train_dir)\n",
    "    train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr><td bgcolor=orange > <font face=\"purisa\" size=8 color='0000ff'>\n",
    " Extended learning\n",
    "</font></td></tr></table>\n",
    "<br>\n",
    "<table><tr><td bgcolor=white > <font face=\"sawasdee\" size=8 color='0000ff'>\n",
    " tf.gfile.DeleeRecursively & tf.gfile.MakeDirs\n",
    "</font></td></tr></table>\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "<font face=\"微软雅黑\" size=6 color='0000ff'> 1. Let us know \"tf.gfile\"<br> </font>\n",
    "<br>\n",
    " \n",
    "<font face=\"微软雅黑\" size=4 color='000000'>\n",
    "The main roles of the tf.gfile module are:\n",
    "    <br>\n",
    "    *:To provide an API that is close to Python's file objects <br>\n",
    "    *:To provide an implementation based on TensorFlow's C++ filesystem API\n",
    "<br> \n",
    "</font>\n",
    "\n",
    "<br>\n",
    "\n",
    "<font face=\"微软雅黑\" size=6 color='0000ff'>2. Let us know \"tf.gfile.DeleeRecursively\"  <br></font>\n",
    "<br>\n",
    "\n",
    "<font face=\"微软雅黑\" size=4 color='000000'>\n",
    "tf.gfile.DeleteRecursively(dirname)\n",
    "\n",
    "Defined in tensorflow/python/lib/io/file_io.py.\n",
    "\n",
    "Deletes everything under dirname recursively.\n",
    "Args:\n",
    "\n",
    "    *dirname: string, a path to a directory\n",
    "\n",
    "Raises:\n",
    "\n",
    "    *errors.OpError: If the operation fails.\n",
    " </font>\n",
    "<br>   \n",
    "    \n",
    "    \n",
    "<font face=\"微软雅黑\" size=6 color='0000ff'> 3. Let us know \"tf.gfile.MakeDirs\" </font><br>  \n",
    "\n",
    "<font face=\"微软雅黑\" size=4 color='#000000'>    \n",
    "tf.gfile.MakeDirs(dirname)\n",
    "\n",
    "Defined in tensorflow/python/lib/io/file_io.py.\n",
    "\n",
    "Creates a directory and all parent/intermediate directories.\n",
    "\n",
    "It succeeds if dirname already exists and is writable.\n",
    "Args:\n",
    "\n",
    "    *dirname: string, name of the directory to be created\n",
    "\n",
    "Raises:\n",
    "\n",
    "    *errors.OpError: If the operation fails.\n",
    "\n",
    " </font>\n",
    "<br>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting 'CUDA_VISIBLE_DEVICES' to use  witch GPU for trianing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "  \"\"\"Train SqueezeDet model\"\"\"\n",
    "  assert FLAGS.dataset == 'KITTI', \\\n",
    "      'Currently only support KITTI dataset'\n",
    "\n",
    "  os.environ['CUDA_VISIBLE_DEVICES'] = FLAGS.gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr><td bgcolor=orange > <font face=\"purisa\" size=8 color='0000ff'>\n",
    " Extended learning\n",
    "</font></td></tr></table>\n",
    "<br>\n",
    "<table><tr><td bgcolor=white > <font face=\"sawasdee\" size=8 color='0000ff'>\n",
    " assert\n",
    "</font></td></tr></table>\n",
    "<br>\n",
    "## Python's assert statement helps you find bugs more quickly and with less pain. This note has some suggestions on good ways to use it. \n",
    "\n",
    "## For example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Currently only support KITTI dataset",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-4adefd95b315>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'dd'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'KITTI'\u001b[0m\u001b[0;34m,\u001b[0m       \u001b[0;34m'Currently only support KITTI dataset'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: Currently only support KITTI dataset"
     ]
    }
   ],
   "source": [
    "dataset = 'dd'\n",
    "assert dataset == 'KITTI', \\\n",
    "      'Currently only support KITTI dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define operations and tensors use tf.Graph().as_default()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Graph().as_default():"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr><td bgcolor=orange > <font face=\"purisa\" size=8 color='0000ff'>\n",
    " Extended learning\n",
    "</font></td></tr></table>\n",
    "<br>\n",
    "<table><tr><td bgcolor=white > <font face=\"sawasdee\" size=8 color='0000ff'>\n",
    " with\n",
    "</font></td></tr></table>\n",
    "<br>\n",
    "有一些任务，可能事先需要设置，事后做清理工作。对于这种场景，Python的with语句提供了一种非常方便的处理方式。一个很好的例子是文件处理，你需要获取一个文件句柄，从文件中读取数据，然后关闭文件句柄。\n",
    "如果不用with语句，代码如下：\n",
    "\n",
    "file = open(\"/tmp/foo.txt\")\n",
    "data = file.read()\n",
    "file.close()\n",
    "\n",
    "这里有两个问题。一是可能忘记关闭文件句柄；二是文件读取数据发生异常，没有进行任何处理。下面是处理异常的加强版本：\n",
    "\n",
    "file = open(\"/tmp/foo.txt\")\n",
    "try:\n",
    "    data = file.read()\n",
    "finally:\n",
    "    file.close()\n",
    "\n",
    "虽然这段代码运行良好，但是太冗长了。这时候就是with一展身手的时候了。除了有更优雅的语法，with还可以很好的处理上下文环境产生的异常。下面是with版本的代码：\n",
    "\n",
    "with open(\"/tmp/foo.txt\") as file:\n",
    "    data = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify the parameters to tune, such as image width, image height, batch size, weight decay...\n",
    "## Specify the Layer imformation of SqueezeDet model\n",
    "## kitti_squeezeDet_config() from ./scr/config/kitti_squeezeDet_config.py\n",
    "\n",
    "### def kitti_squeezeDet_config():\n",
    "###  \"\"\"Specify the parameters to tune below.\"\"\"\n",
    "###  mc                       = base_model_config('KITTI')\n",
    "\n",
    "###  mc.IMAGE_WIDTH           = 1248\n",
    "###  mc.IMAGE_HEIGHT          = 384\n",
    "###  mc.BATCH_SIZE            = 20\n",
    "\n",
    "###  mc.WEIGHT_DECAY          = 0.0001\n",
    "###  mc.LEARNING_RATE         = 0.01\n",
    "###  mc.DECAY_STEPS           = 10000\n",
    "###  mc.MAX_GRAD_NORM         = 1.0\n",
    "###  mc.MOMENTUM              = 0.9\n",
    "###  mc.LR_DECAY_FACTOR       = 0.5\n",
    "\n",
    "###   mc.LOSS_COEF_BBOX        = 5.0\n",
    "###   mc.LOSS_COEF_CONF_POS    = 75.0\n",
    "###   mc### .LOSS_COEF_CONF_NEG    = 100.0\n",
    "###   mc.LOS### S_COEF_CLASS       = 1.0\n",
    "\n",
    "###   mc.PLOT_PROB_THRESH      = 0.4\n",
    "###   mc.NMS_THRESH            = 0.4\n",
    "###   mc.PROB_THRESH           = 0.005\n",
    "###   mc.TOP_N_DETECTION       = 64\n",
    "\n",
    "###   mc.DATA_AUGMENTATION     = True\n",
    "###   mc.DRIFT_X               = 150\n",
    "###   mc.DRIFT_Y               = 100\n",
    "###   mc.EXCLUDE_HARD_EXAMPLES = False\n",
    "\n",
    "###   mc.ANCHOR_BOX            = set_anchors(mc)\n",
    "###   mc.ANCHORS               = len(mc.ANCHOR_BOX)\n",
    "###   mc.ANCHOR_PER_GRID       = 9\n",
    "\n",
    "###   return mc\n",
    "\n",
    "# classes and classes names form base_mode_config('KITTI')\n",
    "\n",
    "## SqueezeDet() from ./scr/nets/SqueezeDet\n",
    "\n",
    "###   self.caffemodel_weight = joblib.load(mc.PRETRAINED_MODEL_PATH)\n",
    "\n",
    "###    conv1 = self._conv_layer(\n",
    "###        'conv1', self.image_input, filters=64, size=3, stride=2,\n",
    "###        padding='SAME', freeze=True)\n",
    "###    pool1 = self._pooling_layer(\n",
    "###        'pool1', conv1, size=3, stride=2, padding='SAME')\n",
    "\n",
    "###    fire2 = self._fire_layer(\n",
    "###        'fire2', pool1, s1x1=16, e1x1=64, e3x3=64, freeze=False)\n",
    "###    fire3 = self._fire_layer(\n",
    "###        'fire3', fire2, s1x1=16, e1x1=64, e3x3=64, freeze=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NMS_THRESH': 0.4, 'LOSS_COEF_CONF_NEG': 100.0, 'KEEP_PROB': 0.5, 'IMAGE_WIDTH': 1248, 'NUM_THREAD': 4, 'LOSS_COEF_CONF_POS': 75.0, 'GRID_POOL_HEIGHT': 7, 'PLOT_PROB_THRESH': 0.4, 'TOP_N_DETECTION': 64, 'LOSS_COEF_CLASS': 1.0, 'BGR_MEANS': array([[[ 103.939,  116.779,  123.68 ]]]), 'PROB_THRESH': 0.005, 'BATCH_SIZE': 20, 'DATASET': 'KITTI', 'DEBUG_MODE': False, 'MAX_GRAD_NORM': 1.0, 'EXP_THRESH': 1.0, 'DRIFT_X': 150, 'LEARNING_RATE': 0.01, 'CLASS_NAMES': ['car', 'pedestrian', 'cyclist'], 'LEAKY_COEF': 0.1, 'EXCLUDE_HARD_EXAMPLES': False, 'QUEUE_CAPACITY': 100, 'DATA_AUGMENTATION': True, 'GRID_POOL_WIDTH': 7, 'PRETRAINED_MODEL_PATH': '', 'CLASSES': 3, 'IS_TRAINING': False, 'ANCHOR_BOX': array([[   15.79746835,    15.36      ,    36.        ,    37.        ],\n",
      "       [   15.79746835,    15.36      ,   366.        ,   174.        ],\n",
      "       [   15.79746835,    15.36      ,   115.        ,    59.        ],\n",
      "       ..., \n",
      "       [ 1232.20253165,   368.64      ,   224.        ,   108.        ],\n",
      "       [ 1232.20253165,   368.64      ,    78.        ,   170.        ],\n",
      "       [ 1232.20253165,   368.64      ,    72.        ,    43.        ]]), 'BATCH_NORM_EPSILON': 1e-05, 'LOSS_COEF_BBOX': 5.0, 'LR_DECAY_FACTOR': 0.5, 'ANCHORS': 16848, 'ANCHOR_PER_GRID': 9, 'LOSS_COEF_CONF': 1.0, 'EPSILON': 1e-16, 'LOAD_PRETRAINED_MODEL': True, 'DECAY_STEPS': 10000, 'DRIFT_Y': 100, 'IMAGE_HEIGHT': 384, 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0001}\n",
      "Cannot find fire10/squeeze1x1 in the pretrained model. Use randomly initialized parameters\n",
      "Cannot find fire10/expand1x1 in the pretrained model. Use randomly initialized parameters\n",
      "Cannot find fire10/expand3x3 in the pretrained model. Use randomly initialized parameters\n",
      "Cannot find fire11/squeeze1x1 in the pretrained model. Use randomly initialized parameters\n",
      "Cannot find fire11/expand1x1 in the pretrained model. Use randomly initialized parameters\n",
      "Cannot find fire11/expand3x3 in the pretrained model. Use randomly initialized parameters\n",
      "Cannot find conv12 in the pretrained model. Use randomly initialized parameters\n",
      "INFO:tensorflow:Summary name mean iou is illegal; using mean_iou instead.\n"
     ]
    }
   ],
   "source": [
    "    ## elif FLAGS.net == 'squeezeDet':\n",
    "\n",
    "mc = kitti_squeezeDet_config()\n",
    "print(mc)\n",
    "mc.IS_TRAINING = True\n",
    "mc.PRETRAINED_MODEL_PATH = '../squeezeDet-master/data/SqueezeNet/squeezenet_v1.1.pkl'#FLAGS.pretrained_model_path\n",
    "model = SqueezeDet(mc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "imdb = kitti(FLAGS.image_set, FLAGS.data_path, mc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr><td bgcolor=orange > <font face=\"purisa\" size=8 color='0000ff'>\n",
    " Extended learning\n",
    "</font></td></tr></table>\n",
    "<br>\n",
    "<table><tr><td bgcolor=white > <font face=\"sawasdee\" size=6 color='0000ff'>\n",
    "Read the names and parameters of layers from .ckpt\n",
    "</font></td></tr></table>\n",
    "<br>\n",
    "## Sometimes we have a .cpkt file, and we want to get the names and parameters of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../squeezeDet-master/data/model_checkpoints/squeezeDet/model.ckpt-87000\n",
      "tensor_name fire4/expand1x1/biases\n",
      "tensor_name fire9/expand1x1/kernels/Momentum\n",
      "tensor_name fire6/expand1x1/biases\n",
      "tensor_name fire4/expand3x3/kernels\n",
      "tensor_name fire11/expand3x3/kernels/Momentum\n",
      "tensor_name fire6/expand3x3/biases/Momentum\n",
      "tensor_name fire2/squeeze1x1/biases\n",
      "tensor_name fire2/squeeze1x1/kernels\n",
      "tensor_name fire11/expand3x3/kernels\n",
      "tensor_name fire5/expand1x1/kernels/Momentum\n",
      "tensor_name fire10/expand3x3/biases\n",
      "tensor_name fire7/squeeze1x1/kernels/Momentum\n",
      "tensor_name fire4/squeeze1x1/biases\n",
      "tensor_name fire2/expand1x1/kernels/Momentum\n",
      "tensor_name fire8/squeeze1x1/biases\n",
      "tensor_name fire6/expand1x1/kernels\n",
      "tensor_name fire4/expand1x1/kernels\n",
      "tensor_name fire11/expand3x3/biases/Momentum\n",
      "tensor_name fire10/expand1x1/kernels\n",
      "tensor_name fire4/squeeze1x1/kernels\n",
      "tensor_name fire4/expand3x3/kernels/Momentum\n",
      "tensor_name fire4/squeeze1x1/biases/Momentum\n",
      "tensor_name fire11/expand1x1/kernels\n",
      "tensor_name fire6/expand3x3/kernels/Momentum\n",
      "tensor_name fire7/expand1x1/kernels/Momentum\n",
      "tensor_name fire10/expand1x1/kernels/Momentum\n",
      "tensor_name fire2/squeeze1x1/biases/Momentum\n",
      "tensor_name fire2/expand1x1/biases/Momentum\n",
      "tensor_name fire2/expand3x3/kernels\n",
      "tensor_name fire2/expand3x3/biases/Momentum\n",
      "tensor_name fire4/squeeze1x1/kernels/Momentum\n",
      "tensor_name fire8/squeeze1x1/kernels/Momentum\n",
      "tensor_name fire8/expand3x3/kernels\n",
      "tensor_name fire3/expand1x1/kernels/Momentum\n",
      "tensor_name fire2/expand1x1/biases\n",
      "tensor_name fire5/squeeze1x1/kernels/Momentum\n",
      "tensor_name fire3/squeeze1x1/kernels/Momentum\n",
      "tensor_name fire10/expand1x1/biases/Momentum\n",
      "tensor_name fire8/squeeze1x1/kernels\n",
      "tensor_name fire11/squeeze1x1/kernels/Momentum\n",
      "tensor_name fire8/expand3x3/biases/Momentum\n",
      "tensor_name fire9/expand1x1/kernels\n",
      "tensor_name fire3/expand3x3/kernels\n",
      "tensor_name conv12/kernels\n",
      "tensor_name fire8/expand1x1/biases/Momentum\n",
      "tensor_name fire9/expand1x1/biases\n",
      "tensor_name fire7/squeeze1x1/kernels\n",
      "tensor_name fire11/squeeze1x1/biases\n",
      "tensor_name fire11/expand3x3/biases\n",
      "tensor_name fire3/expand3x3/biases\n",
      "tensor_name fire7/expand3x3/kernels\n",
      "tensor_name fire6/expand1x1/biases/Momentum\n",
      "tensor_name fire7/expand1x1/biases\n",
      "tensor_name fire6/expand3x3/kernels\n",
      "tensor_name fire2/expand3x3/biases\n",
      "tensor_name fire11/expand1x1/biases/Momentum\n",
      "tensor_name fire10/expand3x3/kernels/Momentum\n",
      "tensor_name fire6/squeeze1x1/biases/Momentum\n",
      "tensor_name fire2/squeeze1x1/kernels/Momentum\n",
      "tensor_name fire5/squeeze1x1/biases\n",
      "tensor_name fire5/expand1x1/biases\n",
      "tensor_name fire8/expand1x1/kernels/Momentum\n",
      "tensor_name fire7/expand1x1/kernels\n",
      "tensor_name conv12/biases\n",
      "tensor_name fire3/squeeze1x1/kernels\n",
      "tensor_name fire5/expand3x3/biases\n",
      "tensor_name fire10/squeeze1x1/kernels\n",
      "tensor_name fire4/expand3x3/biases/Momentum\n",
      "tensor_name fire8/expand1x1/biases\n",
      "tensor_name fire8/expand3x3/kernels/Momentum\n",
      "tensor_name fire10/expand1x1/biases\n",
      "tensor_name fire9/expand3x3/kernels\n",
      "tensor_name fire7/expand1x1/biases/Momentum\n",
      "tensor_name fire8/expand3x3/biases\n",
      "tensor_name fire3/expand1x1/kernels\n",
      "tensor_name fire4/expand1x1/biases/Momentum\n",
      "tensor_name fire10/squeeze1x1/biases\n",
      "tensor_name fire6/expand3x3/biases\n",
      "tensor_name fire5/expand3x3/biases/Momentum\n",
      "tensor_name fire9/squeeze1x1/kernels\n",
      "tensor_name fire7/expand3x3/biases/Momentum\n",
      "tensor_name fire7/squeeze1x1/biases\n",
      "tensor_name fire3/expand3x3/kernels/Momentum\n",
      "tensor_name fire4/expand3x3/biases\n",
      "tensor_name fire8/expand1x1/kernels\n",
      "tensor_name fire9/squeeze1x1/kernels/Momentum\n",
      "tensor_name fire9/expand3x3/biases/Momentum\n",
      "tensor_name fire10/expand3x3/biases/Momentum\n",
      "tensor_name conv1/biases\n",
      "tensor_name conv12/biases/Momentum\n",
      "tensor_name fire5/expand3x3/kernels\n",
      "tensor_name fire2/expand3x3/kernels/Momentum\n",
      "tensor_name fire7/squeeze1x1/biases/Momentum\n",
      "tensor_name conv1/kernels\n",
      "tensor_name fire9/expand1x1/biases/Momentum\n",
      "tensor_name fire2/expand1x1/kernels\n",
      "tensor_name fire3/expand1x1/biases/Momentum\n",
      "tensor_name fire9/expand3x3/kernels/Momentum\n",
      "tensor_name global_step\n",
      "tensor_name fire10/expand3x3/kernels\n",
      "tensor_name fire3/squeeze1x1/biases\n",
      "tensor_name fire6/squeeze1x1/kernels/Momentum\n",
      "tensor_name fire11/expand1x1/kernels/Momentum\n",
      "tensor_name fire8/squeeze1x1/biases/Momentum\n",
      "tensor_name fire5/squeeze1x1/kernels\n",
      "tensor_name fire3/expand3x3/biases/Momentum\n",
      "tensor_name fire6/expand1x1/kernels/Momentum\n",
      "tensor_name fire4/expand1x1/kernels/Momentum\n",
      "tensor_name fire5/expand3x3/kernels/Momentum\n",
      "tensor_name fire7/expand3x3/biases\n",
      "tensor_name iou\n",
      "tensor_name fire7/expand3x3/kernels/Momentum\n",
      "tensor_name fire6/squeeze1x1/biases\n",
      "tensor_name fire3/expand1x1/biases\n",
      "tensor_name fire5/squeeze1x1/biases/Momentum\n",
      "tensor_name conv12/kernels/Momentum\n",
      "tensor_name fire9/squeeze1x1/biases\n",
      "tensor_name fire6/squeeze1x1/kernels\n",
      "tensor_name fire10/squeeze1x1/biases/Momentum\n",
      "tensor_name fire5/expand1x1/kernels\n",
      "tensor_name fire9/squeeze1x1/biases/Momentum\n",
      "tensor_name fire11/squeeze1x1/biases/Momentum\n",
      "tensor_name fire9/expand3x3/biases\n",
      "tensor_name fire3/squeeze1x1/biases/Momentum\n",
      "tensor_name fire11/expand1x1/biases\n",
      "tensor_name fire11/squeeze1x1/kernels\n",
      "tensor_name fire10/squeeze1x1/kernels/Momentum\n",
      "tensor_name fire5/expand1x1/biases/Momentum\n",
      "[ -9.12857614e-03  -2.08038148e-02   2.65010018e-02 ...,  -2.69910342e-05\n",
      "   2.42647293e-05  -2.81593257e-05] (4463409,) 87001.0 -1.47793996334 0.0182695371489\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.python import pywrap_tensorflow\n",
    "checkpoint_path=\"../squeezeDet-master/data/model_checkpoints/squeezeDet/model.ckpt-87000\"\n",
    "# print(path.getcwdu())\n",
    "print(checkpoint_path)\n",
    "#read data from checkpoint file\n",
    "reader=pywrap_tensorflow.NewCheckpointReader(checkpoint_path)\n",
    "var_to_shape_map=reader.get_variable_to_shape_map()\n",
    "data_print=np.array([])\n",
    "for key in var_to_shape_map:\n",
    "    print('tensor_name',key)\n",
    "    ckpt_data=np.array(reader.get_tensor(key))#cast list to np arrary\n",
    "    ckpt_data=ckpt_data.flatten()#flatten list\n",
    "    data_print=np.append(data_print,ckpt_data,axis=0)\n",
    " \n",
    " \n",
    "print(data_print,data_print.shape,np.max(data_print),np.min(data_print),np.mean(data_print))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72,)\n",
      "[  2.67206039e-02   3.06304753e-01  -3.33028406e-01   6.52096748e-01\n",
      "  -2.89292663e-01  -3.62792015e-01   1.01041019e+00  -7.65033066e-01\n",
      "  -2.45314330e-01   6.69764757e-01  -6.41299248e-01  -2.83657033e-02\n",
      "  -3.77765059e-01   5.24263263e-01  -1.46511480e-01   1.41501911e-02\n",
      "  -1.65120184e-01   1.50972545e-01   7.24290848e-01  -4.63925958e-01\n",
      "  -2.60360301e-01  -3.25740665e-01   2.44966179e-01   8.07715505e-02\n",
      "   1.16953218e+00  -8.97056758e-01  -2.72386789e-01  -8.33941102e-02\n",
      "  -1.32141984e+00  -9.77066934e-01  -1.27273750e+00  -3.48524332e-01\n",
      "  -1.08383143e+00  -1.42020071e+00  -8.32717836e-01  -5.30676067e-01\n",
      "  -2.60286499e-02  -3.37756872e-02  -4.75859255e-01  -2.89946273e-02\n",
      "   2.20363820e-03   2.84101702e-02   6.77693561e-02   1.16262794e-01\n",
      "   7.13630253e-03  -1.20983338e-02  -1.49468407e-01   1.44799636e-03\n",
      "  -1.53781287e-03   1.75991151e-02  -7.02554360e-02   9.85950753e-02\n",
      "  -3.72283310e-02   1.90018322e-02  -2.42904842e-01   3.53472680e-02\n",
      "   1.04729421e-02   6.30156649e-03  -9.09068510e-02   8.56361762e-02\n",
      "  -7.92960927e-05   6.69544423e-03   1.66168045e-02   7.03814551e-02\n",
      "  -1.13923894e-02   1.57915335e-02  -1.62527904e-01   4.17497493e-02\n",
      "  -8.84042773e-03  -2.28384361e-02  -1.27816379e-01   7.76060149e-02]\n"
     ]
    }
   ],
   "source": [
    "parameter_fire4 = reader.get_tensor(\"conv12/biases\")\n",
    "print(parameter_fire4.shape)\n",
    "print(parameter_fire4[:,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print the values of the classification and the anchor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anchor number: 9\n",
      "Classes number: 3\n",
      "Classes number names ['car', 'pedestrian', 'cyclist']\n"
     ]
    }
   ],
   "source": [
    "print(\"Anchor number:\", mc.ANCHOR_PER_GRID)\n",
    "print(\"Classes number:\", mc.CLASSES)\n",
    "print(\"Classes number names\", mc.CLASS_NAMES)\n",
    "num_output = mc.ANCHOR_PER_GRID * (mc.CLASSES + 1 + 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detail of kitti function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os \n",
    "import numpy as np\n",
    "import subprocess\n",
    "\n",
    "from dataset.imdb import imdb\n",
    "from utils.util import bbox_transform_inv, batch_iou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## class kitti\n",
    "    Get the 3 parameters,\n",
    "        image_set: train or val\n",
    "        data_path: the path of the dataset\n",
    "        mc: the parameters of model\n",
    "    zip the classes name and calsses number\n",
    "    dic creat the dictionary\n",
    "    Get the training files name from training.txt\n",
    "    Get the gound truch form label flies and transform (xmin, ymin, xmax, ymax) to (cx, cy, w, h)\n",
    "         cls  = obj[0]\n",
    "         xmin = float(obj[4])\n",
    "         ymin = float(obj[5])\n",
    "         xmax = float(obj[6])\n",
    "         ymax = float(obj[7])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class kitti(imdb):\n",
    "    def __init__(self, image_set, data_path, mc):\n",
    "        imdb.__init__(self, 'kitti_'+image_set, mc)\n",
    "        self._image_set = image_set\n",
    "        self._data_root_path = data_path\n",
    "        self._image_path = os.path.join(self._data_root_path, 'training', 'image_2')\n",
    "        self._label_path = os.path.join(self._data_root_path, 'training', 'label_2')\n",
    "        self._classes = self.mc.CLASS_NAMES\n",
    "        self._class_to_idx = dict(zip(self.classes, xrange(self.num_classes)))\n",
    "        print(\"self.num_classes:\", self.num_classes)\n",
    "        print(\"self.classes:\", self.classes)\n",
    "        print(\"class to idx:\", self._class_to_idx)\n",
    "    \n",
    "        # a list of string indices of images in the directory\n",
    "        self._image_idx = self._load_image_set_idx() \n",
    "        print(\"image_class:\", self._image_idx)\n",
    "    # a dict of image_idx -> [[cx, cy, w, h, cls_idx]]. x,y,w,h are not divided by\n",
    "    # the image width and height\n",
    "        self._rois = self._load_kitti_annotation()\n",
    "\n",
    "    ## batch reader ##\n",
    "        self._perm_idx = None\n",
    "        self._cur_idx = 0\n",
    "    # TODO(bichen): add a random seed as parameter\n",
    "        self._shuffle_image_idx()\n",
    "        self._eval_tool = '../dataset/kitti-eval/cpp/evaluate_object'\n",
    "        \n",
    "    def _load_image_set_idx(self):\n",
    "        image_set_file = os.path.join(\n",
    "            self._data_root_path, 'ImageSets', self._image_set+'.txt')\n",
    "        assert os.path.exists(image_set_file), \\\n",
    "            'File does not exist: {}'.format(image_set_file)\n",
    "\n",
    "        with open(image_set_file) as f:\n",
    "            image_idx = [x.strip() for x in f.readlines()]\n",
    "        return image_idx\n",
    "\n",
    "    def _image_path_at(self, idx):\n",
    "        image_path = os.path.join(self._image_path, idx+'.png')\n",
    "        assert os.path.exists(image_path), \\\n",
    "            'Image does not exist: {}'.format(image_path)\n",
    "        return image_path\n",
    "\n",
    "    def _load_kitti_annotation(self):\n",
    "        def _get_obj_level(obj):\n",
    "            height = float(obj[7]) - float(obj[5]) + 1\n",
    "            truncation = float(obj[1])\n",
    "            occlusion = float(obj[2])\n",
    "            if height >= 40 and truncation <= 0.15 and occlusion <= 0:\n",
    "                return 1\n",
    "            elif height >= 25 and truncation <= 0.3 and occlusion <= 1:\n",
    "                return 2\n",
    "            elif height >= 25 and truncation <= 0.5 and occlusion <= 2:\n",
    "                return 3\n",
    "            else:\n",
    "                return 4\n",
    "\n",
    "        idx2annotation = {}\n",
    "        for index in self._image_idx:\n",
    "            filename = os.path.join(self._label_path, index+'.txt')\n",
    "            with open(filename, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "            f.close()\n",
    "            bboxes = []\n",
    "            for line in lines:\n",
    "                obj = line.strip().split(' ')\n",
    "                print(\"obj:\", obj)\n",
    "                try:\n",
    "                    cls = self._class_to_idx[obj[0].lower().strip()]\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "                if self.mc.EXCLUDE_HARD_EXAMPLES and _get_obj_level(obj) > 3:\n",
    "                    continue\n",
    "                xmin = float(obj[4])\n",
    "                ymin = float(obj[5])\n",
    "                xmax = float(obj[6])\n",
    "                ymax = float(obj[7])\n",
    "                assert xmin >= 0.0 and xmin <= xmax, \\\n",
    "                   'Invalid bounding box x-coord xmin {} or xmax {} at {}.txt' \\\n",
    "                    .format(xmin, xmax, index)\n",
    "                assert ymin >= 0.0 and ymin <= ymax, \\\n",
    "                   'Invalid bounding box y-coord ymin {} or ymax {} at {}.txt' \\\n",
    "                    .format(ymin, ymax, index)\n",
    "                x, y, w, h = bbox_transform_inv([xmin, ymin, xmax, ymax])\n",
    "                bboxes.append([x, y, w, h, cls])\n",
    "                print(\"FileName:\", filename)\n",
    "                print(\"bboxes\", bboxes)\n",
    "\n",
    "            idx2annotation[index] = bboxes\n",
    "\n",
    "        return idx2annotation\n",
    "\n",
    "    def evaluate_detections(self, eval_dir, global_step, all_boxes):\n",
    "        \"\"\"Evaluate detection results.\n",
    "        Args:\n",
    "        eval_dir: directory to write evaluation logs\n",
    "        global_step: step of the checkpoint\n",
    "        all_boxes: all_boxes[cls][image] = N x 5 arrays of \n",
    "        [xmin, ymin, xmax, ymax, score]\n",
    "        Returns:\n",
    "        aps: array of average precisions.\n",
    "        names: class names corresponding to each ap\n",
    "        \"\"\"\n",
    "        det_file_dir = os.path.join(\n",
    "             eval_dir, 'detection_files_{:s}'.format(global_step), 'data')\n",
    "        if not os.path.isdir(det_file_dir):\n",
    "            os.makedirs(det_file_dir)\n",
    "\n",
    "        for im_idx, index in enumerate(self._image_idx):\n",
    "            filename = os.path.join(det_file_dir, index+'.txt')\n",
    "            with open(filename, 'wt') as f:\n",
    "                for cls_idx, cls in enumerate(self._classes):\n",
    "                    dets = all_boxes[cls_idx][im_idx]\n",
    "                    for k in xrange(len(dets)):\n",
    "                        f.write(\n",
    "                            '{:s} -1 -1 0.0 {:.2f} {:.2f} {:.2f} {:.2f} 0.0 0.0 0.0 0.0 0.0 '\n",
    "                            '0.0 0.0 {:.3f}\\n'.format(\n",
    "                                cls.lower(), dets[k][0], dets[k][1], dets[k][2], dets[k][3],\n",
    "                                dets[k][4])\n",
    "                        )\n",
    "\n",
    "        cmd = self._eval_tool + ' ' \\\n",
    "              + os.path.join(self._data_root_path, 'training') + ' ' \\\n",
    "              + os.path.join(self._data_root_path, 'ImageSets',\n",
    "                             self._image_set+'.txt') + ' ' \\\n",
    "              + os.path.dirname(det_file_dir) + ' ' + str(len(self._image_idx))\n",
    "\n",
    "        print('Running: {}'.format(cmd))\n",
    "        status = subprocess.call(cmd, shell=True)\n",
    "\n",
    "        aps = []\n",
    "        names = []\n",
    "        for cls in self._classes:\n",
    "            det_file_name = os.path.join(\n",
    "                os.path.dirname(det_file_dir), 'stats_{:s}_ap.txt'.format(cls))\n",
    "            if os.path.exists(det_file_name):\n",
    "                with open(det_file_name, 'r') as f:\n",
    "                    lines = f.readlines()\n",
    "                assert len(lines) == 3, \\\n",
    "                   'Line number of {} should be 3'.format(det_file_name)\n",
    "\n",
    "                aps.append(float(lines[0].split('=')[1].strip()))\n",
    "                aps.append(float(lines[1].split('=')[1].strip()))\n",
    "                aps.append(float(lines[2].split('=')[1].strip()))\n",
    "            else:\n",
    "                aps.extend([0.0, 0.0, 0.0])\n",
    "\n",
    "            names.append(cls+'_easy')\n",
    "            names.append(cls+'_medium')\n",
    "            names.append(cls+'_hard')\n",
    "\n",
    "        return aps, names\n",
    "\n",
    "    def do_detection_analysis_in_eval(self, eval_dir, global_step):\n",
    "        det_file_dir = os.path.join(\n",
    "            eval_dir, 'detection_files_{:s}'.format(global_step), 'data')\n",
    "        det_error_dir = os.path.join(\n",
    "            eval_dir, 'detection_files_{:s}'.format(global_step),\n",
    "            'error_analysis')\n",
    "        if not os.path.exists(det_error_dir):\n",
    "            os.makedirs(det_error_dir)\n",
    "        det_error_file = os.path.join(det_error_dir, 'det_error_file.txt')\n",
    "\n",
    "        stats = self.analyze_detections(det_file_dir, det_error_file)\n",
    "        ims = self.visualize_detections(\n",
    "            image_dir=self._image_path,\n",
    "            image_format='.png',\n",
    "            det_error_file=det_error_file,\n",
    "            output_image_dir=det_error_dir,\n",
    "            num_det_per_type=10\n",
    "        )\n",
    "\n",
    "        return stats, ims\n",
    "\n",
    "    def analyze_detections(self, detection_file_dir, det_error_file):\n",
    "        def _save_detection(f, idx, error_type, det, score):\n",
    "            f.write(\n",
    "                '{:s} {:s} {:.1f} {:.1f} {:.1f} {:.1f} {:s} {:.3f}\\n'.format(\n",
    "                    idx, error_type,\n",
    "                    det[0]-det[2]/2., det[1]-det[3]/2.,\n",
    "                    det[0]+det[2]/2., det[1]+det[3]/2.,\n",
    "                    self._classes[int(det[4])], \n",
    "                    score\n",
    "                    )\n",
    "             )\n",
    "\n",
    "    # load detections\n",
    "        self._det_rois = {}\n",
    "        for idx in self._image_idx:\n",
    "            det_file_name = os.path.join(detection_file_dir, idx+'.txt')\n",
    "            with open(det_file_name) as f:\n",
    "                lines = f.readlines()\n",
    "            f.close()\n",
    "            bboxes = []\n",
    "            for line in lines:\n",
    "                obj = line.strip().split(' ')\n",
    "                cls = self._class_to_idx[obj[0].lower().strip()]\n",
    "                xmin = float(obj[4])\n",
    "                ymin = float(obj[5])\n",
    "                xmax = float(obj[6])\n",
    "                ymax = float(obj[7])\n",
    "                score = float(obj[-1])\n",
    "                print(\"file_name:\", det_file_name)\n",
    "                print(\"cls:\", cls)\n",
    "                print(\"xmin:\", xmin, \"ymin:\", ymin, \"xmax:\", xmax, \"ymax:\", ymax)\n",
    "                print(\"score:\",score)\n",
    "\n",
    "                x, y, w, h = bbox_transform_inv([xmin, ymin, xmax, ymax])\n",
    "                bboxes.append([x, y, w, h, cls, score])\n",
    "            bboxes.sort(key=lambda x: x[-1], reverse=True)\n",
    "            self._det_rois[idx] = bboxes\n",
    "\n",
    "    # do error analysis\n",
    "        num_objs = 0.\n",
    "        num_dets = 0.\n",
    "        num_correct = 0.\n",
    "        num_loc_error = 0.\n",
    "        num_cls_error = 0.\n",
    "        num_bg_error = 0.\n",
    "        num_repeated_error = 0.\n",
    "        num_detected_obj = 0.\n",
    "\n",
    "        with open(det_error_file, 'w') as f:\n",
    "            for idx in self._image_idx:\n",
    "                gt_bboxes = np.array(self._rois[idx])\n",
    "                num_objs += len(gt_bboxes)\n",
    "                detected = [False]*len(gt_bboxes)\n",
    "\n",
    "                det_bboxes = self._det_rois[idx]\n",
    "                if len(gt_bboxes) < 1:\n",
    "                    continue\n",
    "\n",
    "                for i, det in enumerate(det_bboxes):\n",
    "                    if i < len(gt_bboxes):\n",
    "                        num_dets += 1\n",
    "                    ious = batch_iou(gt_bboxes[:, :4], det[:4])\n",
    "                    max_iou = np.max(ious)\n",
    "                    gt_idx = np.argmax(ious)\n",
    "                    if max_iou > 0.1:\n",
    "                        if gt_bboxes[gt_idx, 4] == det[4]:\n",
    "                            if max_iou >= 0.5:\n",
    "                                if i < len(gt_bboxes):\n",
    "                                    if not detected[gt_idx]:\n",
    "                                        num_correct += 1\n",
    "                                        detected[gt_idx] = True\n",
    "                                    else:\n",
    "                                        num_repeated_error += 1\n",
    "                            else:\n",
    "                                if i < len(gt_bboxes):\n",
    "                                    num_loc_error += 1\n",
    "                                    _save_detection(f, idx, 'loc', det, det[5])\n",
    "                        else:\n",
    "                            if i < len(gt_bboxes):\n",
    "                                num_cls_error += 1\n",
    "                                _save_detection(f, idx, 'cls', det, det[5])\n",
    "                    else:\n",
    "                        if i < len(gt_bboxes):\n",
    "                            num_bg_error += 1\n",
    "                            _save_detection(f, idx, 'bg', det, det[5])\n",
    "\n",
    "                for i, gt in enumerate(gt_bboxes):\n",
    "                    if not detected[i]:\n",
    "                        _save_detection(f, idx, 'missed', gt, -1.0)\n",
    "                num_detected_obj += sum(detected)\n",
    "        f.close()\n",
    "\n",
    "        print ('Detection Analysis:')\n",
    "        print ('    Number of detections: {}'.format(num_dets))\n",
    "        print ('    Number of objects: {}'.format(num_objs))\n",
    "        print ('    Percentage of correct detections: {}'.format(\n",
    "            num_correct/num_dets))\n",
    "        print ('    Percentage of localization error: {}'.format(\n",
    "            num_loc_error/num_dets))\n",
    "        print ('    Percentage of classification error: {}'.format(\n",
    "            num_cls_error/num_dets))\n",
    "        print ('    Percentage of background error: {}'.format(\n",
    "            num_bg_error/num_dets))\n",
    "        print ('    Percentage of repeated detections: {}'.format(\n",
    "            num_repeated_error/num_dets))\n",
    "        print ('    Recall: {}'.format(\n",
    "            num_detected_obj/num_objs))\n",
    "\n",
    "        out = {}\n",
    "        out['num of detections'] = num_dets\n",
    "        out['num of objects'] = num_objs\n",
    "        out['% correct detections'] = num_correct/num_dets\n",
    "        out['% localization error'] = num_loc_error/num_dets\n",
    "        out['% classification error'] = num_cls_error/num_dets\n",
    "        out['% background error'] = num_bg_error/num_dets\n",
    "        out['% repeated error'] = num_repeated_error/num_dets\n",
    "        out['% recall'] = num_detected_obj/num_objs\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb = kitti('train', '../data/KITTI', mc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.kitti object at 0x7f105d3ac850>\n"
     ]
    }
   ],
   "source": [
    "print(imdb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr><td bgcolor=orange > <font face=\"purisa\" size=8 color='0000ff'>\n",
    " Extended learning\n",
    "</font></td></tr></table>\n",
    "<br>\n",
    "<table><tr><td bgcolor=white > <font face=\"sawasdee\" size=6 color='0000ff'>\n",
    "range vs xrange\n",
    "</font></td></tr></table>\n",
    "<br>\n",
    "## For the most part, xrange and range are the exact same in terms of functionality. They both provide a way to generate a list of integers for you to use, however you please. The only difference is that range returns a Python list object and xrange returns an xrange object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: xrange(0, 10, 2)\n",
      "type of a: <type 'xrange'>\n",
      "member of a: a[0]= 0 a[1]= 2\n",
      "b: [0, 2, 4, 6, 8]\n",
      "type of b: <type 'list'>\n",
      "member of b: b[0]= 0 b[1]= 2\n",
      "c: [0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "a = xrange(0,10,2)\n",
    "print(\"a:\", a)\n",
    "print(\"type of a:\", type(a))\n",
    "print(\"member of a:\", \"a[0]=\", a[0], \"a[1]=\", a[1])\n",
    "b = range(0,10,2)\n",
    "print(\"b:\", b)\n",
    "print(\"type of b:\", type(b))\n",
    "print(\"member of b:\",\"b[0]=\", b[0],\"b[1]=\", b[1])\n",
    "\n",
    "c = range(3)\n",
    "print(\"c:\", c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr><td bgcolor=orange > <font face=\"purisa\" size=8 color='0000ff'>\n",
    " Extended learning\n",
    "</font></td></tr></table>\n",
    "<br>\n",
    "<table><tr><td bgcolor=white > <font face=\"sawasdee\" size=6 color='0000ff'>\n",
    "zip\n",
    "</font></td></tr></table>\n",
    "<br>\n",
    "## zip takes n number of iterables and returns list of tuples. ith element of the tuple is created using the ith element from each of the iterables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'a'), (2, 'b'), (3, 'c'), (4, 'd'), (5, 'e')]\n"
     ]
    }
   ],
   "source": [
    "list_a = [1, 2, 3, 4, 5]\n",
    "list_b = ['a', 'b', 'c', 'd', 'e']\n",
    "\n",
    "zipped_list = zip(list_a, list_b)\n",
    "\n",
    "print (zipped_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "鹅鹅鹅\n",
      "曲项向天歌\n",
      "锄禾日当午\n",
      "春种一粒粟\n",
      "0 鹅鹅鹅\n",
      "1 曲项向天歌\n",
      "2 锄禾日当午\n",
      "3 春种一粒粟\n"
     ]
    }
   ],
   "source": [
    "l = ['鹅鹅鹅', '曲项向天歌', '锄禾日当午', '春种一粒粟']\n",
    "\n",
    "for i in l:\n",
    "    print(i)\n",
    "\n",
    "# 可以获取下表，enumerate每次循环可以得到下表及元素\n",
    "for i, v in enumerate(l):\n",
    "    print(i, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a apple\n",
      "c car\n",
      "b banana\n",
      "d desk\n",
      "a apple\n",
      "c car\n",
      "b banana\n",
      "d desk\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "[2, 4, 6, 8, 10, 12, 14, 16, 18, 20]\n",
      "[1, 4, 9, 16, 25, 36, 49, 64, 81, 100]\n",
      "['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
      "[2, 4, 6, 8, 10]\n"
     ]
    }
   ],
   "source": [
    "d = {'a':'apple', 'b':'banana', 'c':'car', 'd': 'desk'}\n",
    "\n",
    "for key in d:\n",
    "    # 遍历字典时遍历的是键\n",
    "    print(key, d.get(key))\n",
    "\n",
    "# for key, value in d.items():\n",
    "# 上下两种方式等价 d.items() <=> dict.items(d)\n",
    "for key, value in dict.items(d):\n",
    "    print(key, value)\n",
    "\n",
    "print([i for i in range(1, 11)])\n",
    "print([i*2 for i in range(1, 11)])\n",
    "print([i*i for i in range(1, 11)])\n",
    "print([str(i) for i in range(1, 11)])\n",
    "print([i for i in range(1, 11) if i % 2 == 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr><td bgcolor=orange > <font face=\"purisa\" size=8 color='0000ff'>\n",
    " Extended learning\n",
    "</font></td></tr></table>\n",
    "<br>\n",
    "<table><tr><td bgcolor=white > <font face=\"sawasdee\" size=6 color='0000ff'>\n",
    "strip\n",
    "</font></td></tr></table>\n",
    "<br>\n",
    "## The strip() removes characters from both left and right based on the argument (a string specifying the set of characters to be removed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xoxo love xoxo\n",
      "xoxo love xox\n",
      " xoxo love xoxo   \n",
      "droid is awesome\n"
     ]
    }
   ],
   "source": [
    "b = 'rabc\\t\\n'\n",
    "#print(b.strip()) # Remove '\\n', '\\r',  '\\t',  ' '\n",
    "print(b.strip('r'))\n",
    "string = ' xoxo love xoxo   '\n",
    "\n",
    "# Leading whitepsace are removed\n",
    "print(string.strip())\n",
    "\n",
    "print(string.strip(' o'))\n",
    "\n",
    "# Argument doesn't contain space\n",
    "# No characters are removed.\n",
    "print(string.strip('sti'))\n",
    "\n",
    "string = 'android is awesome'\n",
    "print(string.strip('an'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr><td bgcolor=orange > <font face=\"purisa\" size=8 color='0000ff'>\n",
    " Extended learning\n",
    "</font></td></tr></table>\n",
    "<br>\n",
    "<table><tr><td bgcolor=white > <font face=\"sawasdee\" size=6 color='0000ff'>\n",
    "lower\n",
    "</font></td></tr></table>\n",
    "<br>\n",
    "## The method lower() returns a copy of the string in which all case-based characters have been lowercased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this should be lowercase!\n",
      "th!s sh0uld b3 l0w3rcas3!\n"
     ]
    }
   ],
   "source": [
    "# example string\n",
    "string = \"THIS SHOULD BE LOWERCASE!\"\n",
    "print(string.lower())\n",
    "\n",
    "# string with numbers\n",
    "# all alphabets whould be lowercase\n",
    "string = \"Th!s Sh0uLd B3 L0w3rCas3!\"\n",
    "print(string.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr><td bgcolor=orange > <font face=\"purisa\" size=8 color='0000ff'>\n",
    " Extended learning\n",
    "</font></td></tr></table>\n",
    "<br>\n",
    "<table><tr><td bgcolor=white > <font face=\"sawasdee\" size=6 color='0000ff'>\n",
    "np.arange vs np.range\n",
    "</font></td></tr></table>\n",
    "<br>\n",
    "## （1）range()和xrange()函数\n",
    "\n",
    "    在 python 2.x 版本中，同时存在range（）和xrange（）函数，其中，range()返回值是一个列表，xrange（）返回值是一个迭代值；\n",
    "    在 python 3.x 版本中，取消了xrange（）的定义，仅保留了range（）函数，且range（）函数的返回值也改为迭代值；\n",
    "    xrange和range的语法格式相同。\n",
    "\n",
    "语法：\n",
    "- 格式1: range(stop)\n",
    "- 格式2: range(start, stop[, step])\n",
    "- 返回值：range 对象\n",
    "- start表示起始值（正整数/负整数），stop表示终止值（正整数/负整数），step表示步进值（默认为1，正整数/负整数）；\n",
    "- 当输入参数只有stop时，起始值是0。\n",
    "- 无论如何配置，返回值中均不包括stop值。\n",
    "- 如果要将生成的range（）对象变为列表就需要利用list(range(..))方式；\n",
    "\n",
    "\n",
    "\n",
    "## （2）range()和np.arange()函数\n",
    "\n",
    "    arange()是Numpy库中的函数，其返回值是数组对象，常用于循环；\n",
    "    range()是python的内置函数，其返回值是range对象（迭代值），可用于生成秩为1的数组\n",
    "\n",
    "语法：\n",
    "- 格式：arange([start,] stop[, step,], dtype=None)\n",
    "- 在给定的范围内返回均匀间隔的值，其中step可以为小数；\n",
    "- 该函数生产的数组范围是[start,stop)，即不包括stop值；\n",
    "- start表示起始值（默认为0,可正，可负,可小数），stop表示终止值（正/负数，可小数），step步进值（正/负数，可小数）；\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr><td bgcolor=orange > <font face=\"purisa\" size=8 color='0000ff'>\n",
    " Extended learning\n",
    "</font></td></tr></table>\n",
    "<br>\n",
    "<table><tr><td bgcolor=white > <font face=\"sawasdee\" size=6 color='0000ff'>\n",
    "np.random.shuffle vs np.random.permutation\n",
    "</font></td></tr></table>\n",
    "<br>\n",
    "##  numpy.random.shuffle(x)\n",
    "\n",
    "    Modify a sequence in-place by shuffling its contents.\n",
    "\n",
    "    This function only shuffles the array along the first axis of a multi-dimensional array. The order of sub-arrays is changed but their contents remains the same.\n",
    "    \n",
    "##  numpy.random.permutation(x)\n",
    "\n",
    "    Randomly permute a sequence, or return a permuted range.\n",
    "\n",
    "    If x is a multi-dimensional array, it is only shuffled along its first index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: [ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
      "shuffle a: [ 1  6  9  3 11  4 10  2  5  8  0  7]\n",
      "b: None\n",
      "a: [ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
      "b: [ 6  4  2  3 11  0  9  5 10  8  7  1]\n",
      "permutation a: [ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
      "[[3 4 5]\n",
      " [6 7 8]\n",
      " [0 1 2]]\n",
      "[[0 1 2]\n",
      " [3 4 5]\n",
      " [6 7 8]]\n"
     ]
    }
   ],
   "source": [
    "a = np.arange(12) \n",
    "print (\"a:\", a) \n",
    "b = np.random.shuffle(a) \n",
    "print (\"shuffle a:\", a)\n",
    "print (\"b:\", b)\n",
    "\n",
    "a = np.arange(12) \n",
    "print (\"a:\", a)\n",
    "b = np.random.permutation(a) \n",
    "print (\"b:\", b) \n",
    "print (\"permutation a:\", a)\n",
    "\n",
    "arr = np.arange(9).reshape((3, 3))\n",
    "print(np.random.permutation(arr))\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr><td bgcolor=orange > <font face=\"purisa\" size=8 color='0000ff'>\n",
    " Extended learning\n",
    "</font></td></tr></table>\n",
    "<br>\n",
    "<table><tr><td bgcolor=white > <font face=\"sawasdee\" size=6 color='0000ff'>\n",
    "np.where\n",
    "</font></td></tr></table>\n",
    "<br>\n",
    "## numpy.where() 有两种用法：\n",
    "1. np.where(condition, x, y)\n",
    "\n",
    "满足条件(condition)，输出x，不满足输出y。\n",
    "2. np.where(condition)\n",
    "\n",
    "只有条件 (condition)，没有x和y，则输出满足条件 (即非0) 元素的坐标 (等价于numpy.nonzero)。这里的坐标以tuple的形式给出，通常原数组有多少维，输出的tuple中就包含几个数组，分别对应符合条件元素的各维坐标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1 -1 -1 -1 -1 -1  1  1  1  1]\n",
      "(array([6, 7, 8, 9]),)\n"
     ]
    }
   ],
   "source": [
    "aa = np.arange(10)\n",
    "print(np.where(aa > 5,1,-1))\n",
    "print(np.where(aa > 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to the model parameters to txt\n",
    "      self.model_size_counter.append(\n",
    "          (layer_name, (1+size*size*int(channels))*filters)\n",
    "      )\n",
    " \n",
    "      self.activation_counter.append(\n",
    "          (layer_name, out_shape[1]*out_shape[2]*out_shape[3])\n",
    "      )\n",
    "      \n",
    "       self.flop_counter.append((layer_name, num_flops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(\"../data/KITTI/\", 'model_metrics.txt'), 'w') as f:\n",
    "    f.write('Number of parameter by layer:\\n')\n",
    "    count = 0\n",
    "    for c in model.model_size_counter:\n",
    "        f.write('\\t{}: {}\\n'.format(c[0], c[1]))\n",
    "        count += c[1]\n",
    "    f.write('\\ttotal: {}\\n'.format(count))\n",
    "\n",
    "    count = 0\n",
    "    f.write('\\nActivation size by layer:\\n')\n",
    "    for c in model.activation_counter:\n",
    "        f.write('\\t{}: {}\\n'.format(c[0], c[1]))\n",
    "        count += c[1]\n",
    "    f.write('\\ttotal: {}\\n'.format(count))\n",
    "\n",
    "    count = 0\n",
    "    f.write('\\nNumber of flops by layer:\\n')\n",
    "    for c in model.flop_counter:\n",
    "        f.write('\\t{}: {}\\n'.format(c[0], c[1]))\n",
    "        count += c[1]\n",
    "    f.write('\\ttotal: {}\\n'.format(count))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<nets.squeezeDet.SqueezeDet instance at 0x7f105f159440>\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.global_variables ,Gets an existing variable with these parameters or create a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"conv1/kernels/read:0\", shape=(3, 3, 3, 64), dtype=float32, device=/device:GPU:0)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "variables = tf.global_variables()\n",
    "\n",
    "print (variables[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge_all can save all the summary to disk for tensorboard display. If there is no special requirement, generally use this sentence to display a variety of information during training.\n",
    "\n",
    "## summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.train.get_checkpoint_state find the model file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.train.get_checkpoint_state(FLAGS.train_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 调用 tf.train.Coordinator() 来创建一个线程协调器，用来管理之后在Session中启动的所有线程;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(mc.NUM_THREAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if step % FLAGS.summary_step == 0: #summary_step = 10\n",
    "        feed_dict, image_per_batch, label_per_batch, bbox_per_batch = \\\n",
    "            _load_data(load_to_placeholder=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_batch(self, shuffle=True):\n",
    "    \"\"\"Read a batch of image and bounding box annotations.\n",
    "    Args:\n",
    "      shuffle: whether or not to shuffle the dataset\n",
    "    Returns:\n",
    "      image_per_batch: images. Shape: batch_size x width x height x [b, g, r]\n",
    "      label_per_batch: labels. Shape: batch_size x object_num\n",
    "      delta_per_batch: bounding box deltas. Shape: batch_size x object_num x \n",
    "          [dx ,dy, dw, dh]\n",
    "      aidx_per_batch: index of anchors that are responsible for prediction.\n",
    "          Shape: batch_size x object_num\n",
    "      bbox_per_batch: scaled bounding boxes. Shape: batch_size x object_num x \n",
    "          [cx, cy, w, h]\n",
    "    \"\"\"\n",
    "    mc = self.mc\n",
    "\n",
    "    if shuffle:\n",
    "        # Determine if all dataset was trained, \n",
    "        # if all dataset was trained, it will shuffle dataset for next period\n",
    "        if self._cur_idx + mc.BATCH_SIZE >= len(self._image_idx):\n",
    "            self._shuffle_image_idx()\n",
    "        # Get batch idex from [_cur_idx:_cur_idx + batch_size] to batch_idx\n",
    "        batch_idx = self._perm_idx[self._cur_idx:self._cur_idx+mc.BATCH_SIZE]\n",
    "        self._cur_idx += mc.BATCH_SIZE\n",
    "    else:\n",
    "        if self._cur_idx + mc.BATCH_SIZE >= len(self._image_idx):\n",
    "            batch_idx = self._image_idx[self._cur_idx:] \\\n",
    "                       + self._image_idx[:self._cur_idx + mc.BATCH_SIZE-len(self._image_idx)]\n",
    "            self._cur_idx += mc.BATCH_SIZE - len(self._image_idx)\n",
    "        else:\n",
    "            batch_idx = self._image_idx[self._cur_idx:self._cur_idx+mc.BATCH_SIZE]\n",
    "            self._cur_idx += mc.BATCH_SIZE\n",
    "\n",
    "    image_per_batch = []\n",
    "    label_per_batch = []\n",
    "    bbox_per_batch  = []\n",
    "    delta_per_batch = []\n",
    "    aidx_per_batch  = []\n",
    "    if mc.DEBUG_MODE:\n",
    "        avg_ious = 0.\n",
    "        num_objects = 0.\n",
    "        max_iou = 0.0\n",
    "        min_iou = 1.0\n",
    "        num_zero_iou_obj = 0\n",
    "\n",
    "    for idx in batch_idx:\n",
    "        # load the image\n",
    "        im = cv2.imread(self._image_path_at(idx)).astype(np.float32, copy=False)\n",
    "        # subtract means\n",
    "        im -= mc.BGR_MEANS\n",
    "        # get image size\n",
    "        orig_h, orig_w, _ = [float(v) for v in im.shape]\n",
    "\n",
    "        # load annotations, valus of annotatioms are x y w h ,not minx miny maxx maxy \n",
    "        label_per_batch.append([b[4] for b in self._rois[idx][:]])\n",
    "        gt_bbox = np.array([[b[0], b[1], b[2], b[3]] for b in self._rois[idx][:]])\n",
    "\n",
    "        if mc.DATA_AUGMENTATION:\n",
    "            assert mc.DRIFT_X >= 0 and mc.DRIFT_Y > 0, \\\n",
    "                 'mc.DRIFT_X and mc.DRIFT_Y must be >= 0'\n",
    "\n",
    "            if mc.DRIFT_X > 0 or mc.DRIFT_Y > 0:\n",
    "             # Ensures that gt boundibg box is not cutted out of the image\n",
    "                max_drift_x = min(gt_bbox[:, 0] - gt_bbox[:, 2]/2.0+1)\n",
    "                max_drift_y = min(gt_bbox[:, 1] - gt_bbox[:, 3]/2.0+1)\n",
    "                assert max_drift_x >= 0 and max_drift_y >= 0, 'bbox out of image'\n",
    "\n",
    "                dy = np.random.randint(-mc.DRIFT_Y, min(mc.DRIFT_Y+1, max_drift_y))\n",
    "                dx = np.random.randint(-mc.DRIFT_X, min(mc.DRIFT_X+1, max_drift_x))\n",
    "\n",
    "          # shift bbox\n",
    "                gt_bbox[:, 0] = gt_bbox[:, 0] - dx\n",
    "                gt_bbox[:, 1] = gt_bbox[:, 1] - dy\n",
    "\n",
    "          # distort image\n",
    "                orig_h -= dy\n",
    "                orig_w -= dx\n",
    "                orig_x, dist_x = max(dx, 0), max(-dx, 0)\n",
    "                orig_y, dist_y = max(dy, 0), max(-dy, 0)\n",
    "\n",
    "                distorted_im = np.zeros(\n",
    "                              (int(orig_h), int(orig_w), 3)).astype(np.float32)\n",
    "                distorted_im[dist_y:, dist_x:, :] = im[orig_y:, orig_x:, :]\n",
    "                im = distorted_im\n",
    "\n",
    "        # Flip image with 50% probability\n",
    "            if np.random.randint(2) > 0.5:\n",
    "                im = im[:, ::-1, :]\n",
    "                gt_bbox[:, 0] = orig_w - 1 - gt_bbox[:, 0]\n",
    "\n",
    "      # scale image\n",
    "        im = cv2.resize(im, (mc.IMAGE_WIDTH, mc.IMAGE_HEIGHT))\n",
    "        image_per_batch.append(im)\n",
    "\n",
    "      # scale annotation\n",
    "        x_scale = mc.IMAGE_WIDTH/orig_w\n",
    "        y_scale = mc.IMAGE_HEIGHT/orig_h\n",
    "        gt_bbox[:, 0::2] = gt_bbox[:, 0::2]*x_scale\n",
    "        gt_bbox[:, 1::2] = gt_bbox[:, 1::2]*y_scale\n",
    "        bbox_per_batch.append(gt_bbox)\n",
    "\n",
    "        aidx_per_image, delta_per_image = [], []\n",
    "        aidx_set = set()\n",
    "        for i in range(len(gt_bbox)):\n",
    "            overlaps = batch_iou(mc.ANCHOR_BOX, gt_bbox[i])\n",
    "\n",
    "            aidx = len(mc.ANCHOR_BOX)\n",
    "            for ov_idx in np.argsort(overlaps)[::-1]:\n",
    "                if overlaps[ov_idx] <= 0:\n",
    "                    if mc.DEBUG_MODE:\n",
    "                        min_iou = min(overlaps[ov_idx], min_iou)\n",
    "                        num_objects += 1\n",
    "                        num_zero_iou_obj += 1\n",
    "                    break\n",
    "                if ov_idx not in aidx_set:\n",
    "                    aidx_set.add(ov_idx)\n",
    "                    aidx = ov_idx\n",
    "                    if mc.DEBUG_MODE:\n",
    "                        max_iou = max(overlaps[ov_idx], max_iou)\n",
    "                        min_iou = min(overlaps[ov_idx], min_iou)\n",
    "                        avg_ious += overlaps[ov_idx]\n",
    "                        num_objects += 1\n",
    "                    break\n",
    "\n",
    "            if aidx == len(mc.ANCHOR_BOX): \n",
    "          # even the largeset available overlap is 0, thus, choose one with the\n",
    "          # smallest square distance\n",
    "                dist = np.sum(np.square(gt_bbox[i] - mc.ANCHOR_BOX), axis=1)\n",
    "                for dist_idx in np.argsort(dist):\n",
    "                    if dist_idx not in aidx_set:\n",
    "                        aidx_set.add(dist_idx)\n",
    "                        aidx = dist_idx\n",
    "                        break\n",
    "\n",
    "            box_cx, box_cy, box_w, box_h = gt_bbox[i]\n",
    "            delta = [0]*4\n",
    "            delta[0] = (box_cx - mc.ANCHOR_BOX[aidx][0])/mc.ANCHOR_BOX[aidx][2]\n",
    "            delta[1] = (box_cy - mc.ANCHOR_BOX[aidx][1])/mc.ANCHOR_BOX[aidx][3]\n",
    "            delta[2] = np.log(box_w/mc.ANCHOR_BOX[aidx][2])\n",
    "            delta[3] = np.log(box_h/mc.ANCHOR_BOX[aidx][3])\n",
    "\n",
    "            aidx_per_image.append(aidx)\n",
    "            delta_per_image.append(delta)\n",
    "\n",
    "        delta_per_batch.append(delta_per_image)\n",
    "        aidx_per_batch.append(aidx_per_image)\n",
    "\n",
    "    if mc.DEBUG_MODE:\n",
    "        print ('max iou: {}'.format(max_iou))\n",
    "        print ('min iou: {}'.format(min_iou))\n",
    "        print ('avg iou: {}'.format(avg_ious/num_objects))\n",
    "        print ('number of objects: {}'.format(num_objects))\n",
    "        print ('number of objects with 0 iou: {}'.format(num_zero_iou_obj))\n",
    "\n",
    "    return image_per_batch, label_per_batch, delta_per_batch, \\\n",
    "        aidx_per_batch, bbox_per_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb = kitti('train', '../data/KITTI', mc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read_batch\n",
      "shuffle True\n",
      "self._cur_idx: 0\n",
      "mc.BATCH_SIZE: 5\n",
      "self._image_idx: ['000001', '000002', '000005', '000007', '000009', '000006', '000004', '000008', '000003', '000000']\n",
      "self._image_idx: ['000001', '000002', '000005', '000007', '000009', '000006', '000004', '000008', '000003', '000000']\n",
      "mc.DEBUG_MODE: False\n",
      "name: ../data/KITTI/training/image_2/000007.png\n",
      "orig_h: 375.0 orig_w: 1242.0\n",
      "self._rois[idx]: [[591.025, 200.16500000000002, 52.809999999999945, 51.150000000000006, 0], [497.56999999999994, 191.755, 31.95999999999998, 23.329999999999984, 0], [554.16, 185.17000000000002, 24.220000000000027, 19.23999999999998, 0], [343.605, 195.345, 26.00999999999999, 38.50999999999999, 2]]\n",
      "label_per_batch.append: <built-in method append of list object at 0x7f64c8c00e60>\n",
      "gt_bbox: [[ 591.025  200.165   52.81    51.15 ]\n",
      " [ 497.57   191.755   31.96    23.33 ]\n",
      " [ 554.16   185.17    24.22    19.24 ]\n",
      " [ 343.605  195.345   26.01    38.51 ]]\n",
      "mc.DATA_AUGMENTATION: True\n",
      "dy: 84\n",
      "dx: -111\n",
      "gt_bbox [[ 647.54412417  153.28989691   48.71166297   67.49690722]\n",
      " [ 561.34172949  142.19216495   29.47973392   30.78597938]\n",
      " [ 613.54004435  133.50268041   22.34039911   25.38886598]\n",
      " [ 419.32523282  146.92948454   23.99148559   50.81731959]]\n",
      "bbox_per_batch.append <built-in method append of list object at 0x7f64c8c00560>\n",
      "overlaps [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "overlaps [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "overlaps [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "overlaps [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "name: ../data/KITTI/training/image_2/000001.png\n",
      "orig_h: 375.0 orig_w: 1242.0\n",
      "self._rois[idx]: [[406.22, 192.82999999999998, 37.18000000000001, 22.580000000000013, 0], [683.29, 179.44, 13.379999999999995, 30.980000000000018, 2]]\n",
      "label_per_batch.append: <built-in method append of list object at 0x7f64c8c00e60>\n",
      "gt_bbox: [[ 406.22  192.83   37.18   22.58]\n",
      " [ 683.29  179.44   13.38   30.98]]\n",
      "mc.DATA_AUGMENTATION: True\n",
      "dy: -51\n",
      "dx: 102\n",
      "gt_bbox [[ 913.86442105  219.79042254   40.70231579   20.35380282]\n",
      " [ 610.54568421  207.72056338   14.64757895   27.9256338 ]]\n",
      "bbox_per_batch.append <built-in method append of list object at 0x7f64c8c00560>\n",
      "overlaps [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "overlaps [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "name: ../data/KITTI/training/image_2/000003.png\n",
      "orig_h: 375.0 orig_w: 1242.0\n",
      "self._rois[idx]: [[671.275, 233.77499999999998, 114.06999999999994, 103.98999999999998, 0]]\n",
      "label_per_batch.append: <built-in method append of list object at 0x7f64c8c00e60>\n",
      "gt_bbox: [[ 671.275  233.775  114.07   103.99 ]]\n",
      "mc.DATA_AUGMENTATION: True\n",
      "dy: -61\n",
      "dx: -26\n",
      "gt_bbox [[ 686.27697161  259.61834862  112.27078864   91.58752294]]\n",
      "bbox_per_batch.append <built-in method append of list object at 0x7f64c8c00560>\n",
      "overlaps [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "name: ../data/KITTI/training/image_2/000004.png\n",
      "orig_h: 375.0 orig_w: 1242.0\n",
      "self._rois[idx]: [[313.14, 200.845, 65.51999999999998, 31.49000000000001, 0], [386.125, 195.37, 41.97000000000003, 21.659999999999997, 0]]\n",
      "label_per_batch.append: <built-in method append of list object at 0x7f64c8c00e60>\n",
      "gt_bbox: [[ 313.14   200.845   65.52    31.49 ]\n",
      " [ 386.125  195.37    41.97    21.66 ]]\n",
      "mc.DATA_AUGMENTATION: True\n",
      "dy: 77\n",
      "dx: -120\n",
      "gt_bbox [[ 850.19770925  159.58550336   60.03594714   40.57771812]\n",
      " [ 783.3215859   152.5304698    38.45709251   27.91087248]]\n",
      "bbox_per_batch.append <built-in method append of list object at 0x7f64c8c00560>\n",
      "overlaps [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "overlaps [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "name: ../data/KITTI/training/image_2/000008.png\n",
      "orig_h: 375.0 orig_w: 1242.0\n",
      "self._rois[idx]: [[201.655, 283.685, 403.31, 182.63, 0], [480.175, 275.99, 290.65, 194.10000000000002, 0], [1089.645, 286.195, 304.71000000000004, 177.61, 0], [659.745, 219.16, 124.30999999999995, 85.95999999999998, 0], [767.2149999999999, 189.13, 52.07000000000005, 40.599999999999994, 0], [920.9649999999999, 209.745, 72.88999999999999, 62.870000000000005, 0]]\n",
      "label_per_batch.append: <built-in method append of list object at 0x7f64c8c00e60>\n",
      "gt_bbox: [[  201.655   283.685   403.31    182.63 ]\n",
      " [  480.175   275.99    290.65    194.1  ]\n",
      " [ 1089.645   286.195   304.71    177.61 ]\n",
      " [  659.745   219.16    124.31     85.96 ]\n",
      " [  767.215   189.13     52.07     40.6  ]\n",
      " [  920.965   209.745    72.89     62.87 ]]\n",
      "mc.DATA_AUGMENTATION: True\n",
      "dy: -67\n",
      "dx: -6\n",
      "gt_bbox [[ 1039.345        304.66751131   403.31         158.66497738]\n",
      " [  760.825        297.98226244   290.65         168.62986425]\n",
      " [  151.355        306.8481448    304.71         154.30371041]\n",
      " [  581.255        248.60959276   124.31          74.680181  ]\n",
      " [  473.785        222.520181      52.07          35.27239819]\n",
      " [  320.035        240.43004525    72.89          54.6200905 ]]\n",
      "bbox_per_batch.append <built-in method append of list object at 0x7f64c8c00560>\n",
      "overlaps [ 0.          0.          0.         ...,  0.10498776  0.06620669\n",
      "  0.02523437]\n",
      "overlaps [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "overlaps [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "overlaps [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "overlaps [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "overlaps [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "aa\n"
     ]
    }
   ],
   "source": [
    "#from dataset.imdb import imdb\n",
    "#import imdb\n",
    "mc.BATCH_SIZE = 5 # Originally 20, for the test I was modified to 5\n",
    "image_per_batch, label_per_batch, box_delta_per_batch, aidx_per_batch, \\\n",
    "          bbox_per_batch = imdb.read_batch()\n",
    "#print(\"image_per_batch:\", image_per_batch,\n",
    "#      \"label_per_batch:\", label_per_batch,\n",
    "#      \"box_delta_per_batch:\", box_delta_per_batch,\n",
    "#      \"aidx_per_batch:\", aidx_per_batch,\n",
    "#      \"bbox_per_batch:\", bbox_per_batch)\n",
    "print (\"aa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE：mc.BGR.MEANS is from /config/config.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chengd\n",
      "18\n",
      "python\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "from test import *\n",
    "obj1 = Foo('chengd', 18)\n",
    "obj1.detail()  # Python默认会将obj1传给self参数，即：obj1.detail(obj1)，所以，此时方法内部的 self ＝ obj1，即：self.name 是 chengd ；self.age 是 18\n",
    "  \n",
    "obj2 = Foo('python', 99)\n",
    "obj2.detail() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr><td bgcolor=orange > <font face=\"purisa\" size=8 color='0000ff'>\n",
    " Extended learning\n",
    "</font></td></tr></table>\n",
    "<br>\n",
    "<table><tr><td bgcolor=white > <font face=\"sawasdee\" size=6 color='0000ff'>\n",
    "查看class的成员\n",
    "</font></td></tr></table>\n",
    "<br>\n",
    "## dir(class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ANCHORS', 'ANCHOR_BOX', 'ANCHOR_PER_GRID', 'BATCH_NORM_EPSILON', 'BATCH_SIZE', 'BGR_MEANS', 'CLASSES', 'CLASS_NAMES', 'DATASET', 'DATA_AUGMENTATION', 'DEBUG_MODE', 'DECAY_STEPS', 'DRIFT_X', 'DRIFT_Y', 'EPSILON', 'EXCLUDE_HARD_EXAMPLES', 'EXP_THRESH', 'GRID_POOL_HEIGHT', 'GRID_POOL_WIDTH', 'IMAGE_HEIGHT', 'IMAGE_WIDTH', 'IS_TRAINING', 'KEEP_PROB', 'LEAKY_COEF', 'LEARNING_RATE', 'LOAD_PRETRAINED_MODEL', 'LOSS_COEF_BBOX', 'LOSS_COEF_CLASS', 'LOSS_COEF_CONF', 'LOSS_COEF_CONF_NEG', 'LOSS_COEF_CONF_POS', 'LR_DECAY_FACTOR', 'MAX_GRAD_NORM', 'MOMENTUM', 'NMS_THRESH', 'NUM_THREAD', 'PLOT_PROB_THRESH', 'PRETRAINED_MODEL_PATH', 'PROB_THRESH', 'QUEUE_CAPACITY', 'TOP_N_DETECTION', 'WEIGHT_DECAY', '__class__', '__cmp__', '__contains__', '__delattr__', '__delitem__', '__dict__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'clear', 'copy', 'fromkeys', 'get', 'has_key', 'items', 'iteritems', 'iterkeys', 'itervalues', 'keys', 'pop', 'popitem', 'setdefault', 'update', 'values', 'viewitems', 'viewkeys', 'viewvalues']\n"
     ]
    }
   ],
   "source": [
    "print (dir(mc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   15.79746835    15.36          36.            37.        ]\n",
      " [   15.79746835    15.36         366.           174.        ]\n",
      " [   15.79746835    15.36         115.            59.        ]\n",
      " ..., \n",
      " [ 1232.20253165   368.64         224.           108.        ]\n",
      " [ 1232.20253165   368.64          78.           170.        ]\n",
      " [ 1232.20253165   368.64          72.            43.        ]]\n"
     ]
    }
   ],
   "source": [
    " print (mc.ANCHOR_BOX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['b', 'r', 'u', 'o', 'n'])\n",
      "set(['a', 'b', 'd', 'f'])\n"
     ]
    }
   ],
   "source": [
    "x = set('runoob')\n",
    "print (x)\n",
    "s = set()\n",
    "s = {'a','b','d','b','f','f'}\n",
    "print (s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0, -1,  1,  3,  2])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.maximum([-2, -1, 0, 1, 2], [0, -2, 1, 3, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#8552a1 size= 6>\n",
    "  data_path data_set\n",
    "</font>\n",
    "<hr class=\"docutils\" />\n",
    "\n",
    "<table style=\"width:100%;border-collapse:collapse;background-color:white;\">\n",
    "    <tr style=\"background-color:rgba(30, 200,100, 0.8)\" >\n",
    "      <th> <font size= 5> train.py  </font></th>\n",
    "    </tr>\n",
    " </table>\n",
    " <code>\n",
    "tf.app.flags.DEFINE_string('dataset', 'KITTI',\n",
    "                           \"\"\"Currently only support KITTI dataset.\"\"\")\n",
    "tf.app.flags.DEFINE_string('data_path', '', \"\"\"Root directory of data\"\"\")\n",
    "tf.app.flags.DEFINE_string('image_set', 'train',\n",
    "                           \"\"\" Can be train, trainval, val, or test\"\"\")\n",
    "</code>\n",
    "\n",
    "<table style=\"width:100%;border-collapse:collapse;background-color:white;\">\n",
    "    <tr style=\"background-color:rgba(30, 200,100, 0.8)\" >\n",
    "      <th> <font size= 5> train.py  </font></th>\n",
    "    </tr>\n",
    " </table>\n",
    " <code>\n",
    "def train():\n",
    "    assert FLAGS.dataset == 'KITTI', \\\n",
    "      'Currently only support KITTI dataset'\n",
    "    elif FLAGS.net == 'squeezeDet':\n",
    "      mc = kitti_squeezeDet_config()\n",
    "      mc.IS_TRAINING = True\n",
    "      mc.PRETRAINED_MODEL_PATH = FLAGS.pretrained_model_path\n",
    "      model = SqueezeDet(mc)\n",
    "# kitti_model_config not use\n",
    "</code>\n",
    "\n",
    "\n",
    "<table style=\"width:100%;border-collapse:collapse;background-color:white;\">\n",
    "    <tr style=\"background-color:rgba(30, 200,100, 0.8)\" >\n",
    "      <th> <font size= 5> config.py  </font></th>\n",
    "    </tr>\n",
    " </table>\n",
    " <code>\n",
    "def base_model_config(dataset='PASCAL_VOC'):    \n",
    "  cfg.DATASET = dataset.upper()\n",
    "  if cfg.DATASET == 'PASCAL_VOC':   \n",
    "    cfg.CLASS_NAMES = ('aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',\n",
    "                       'car', 'cat', 'chair', 'cow', 'diningtable', 'dog',\n",
    "                       'horse', 'motorbike', 'person', 'pottedplant', 'sheep',\n",
    "                       'sofa', 'train', 'tvmonitor')\n",
    "  elif cfg.DATASET == 'KITTI':\n",
    "    cfg.CLASS_NAMES = ('car', 'pedestrian', 'cyclist')\n",
    "</code>\n",
    "\n",
    "<table style=\"width:100%;border-collapse:collapse;background-color:white;\">\n",
    "    <tr style=\"background-color:rgba(30, 200,100, 0.8)\" >\n",
    "      <th> <font size= 5> train.py  </font></th>\n",
    "    </tr>\n",
    " </table>\n",
    " <code>\n",
    "imdb = kitti(FLAGS.image_set, FLAGS.data_path, mc)    \n",
    "#imdb = kitti('train', '../data/KITTI', mc)\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#8552a1 size= 6>\n",
    "  training image\n",
    "</font>\n",
    "<hr class=\"docutils\" />\n",
    "\n",
    "<table style=\"width:100%;border-collapse:collapse;background-color:white;\">\n",
    "    <tr style=\"background-color:rgba(30, 200,100, 0.8)\" >\n",
    "      <th> <font size= 5> train.py  </font></th>\n",
    "    </tr>\n",
    " </table>\n",
    " <code>\n",
    "tf.app.flags.DEFINE_string('dataset', 'KITTI',\n",
    "                           \"\"\"Currently only support KITTI dataset.\"\"\")\n",
    "tf.app.flags.DEFINE_string('data_path', '', \"\"\"Root directory of data\"\"\")\n",
    "tf.app.flags.DEFINE_string('image_set', 'train',\n",
    "                           \"\"\" Can be train, trainval, val, or test\"\"\")\n",
    "</code>\n",
    "\n",
    "<table style=\"width:100%;border-collapse:collapse;background-color:white;\">\n",
    "    <tr style=\"background-color:rgba(30, 200,100, 0.8)\" >\n",
    "      <th> <font size= 5> kitty.py  </font></th>\n",
    "    </tr>\n",
    " </table>\n",
    " <code>\n",
    "class kitti(imdb):\n",
    "  def __init__(self, image_set, data_path, mc):\n",
    "    imdb.__init__(self, 'kitti_'+image_set, mc)\n",
    "    self._image_set = image_set\n",
    "    self._data_root_path = data_path\n",
    "    self._image_path = os.path.join(self._data_root_path, 'training', 'image_2')\n",
    "    self._label_path = os.path.join(self._data_root_path, 'training', 'label_2')    \n",
    "  def _image_path_at(self, idx):\n",
    "    image_path = os.path.join(self._image_path, idx+'.png')\n",
    "    assert os.path.exists(image_path), \\\n",
    "        'Image does not exist: {}'.format(image_path)\n",
    "    return image_path\n",
    "</code>\n",
    "\n",
    "<table style=\"width:100%;border-collapse:collapse;background-color:white;\">\n",
    "    <tr style=\"background-color:rgba(30, 200,100, 0.8)\" >\n",
    "      <th> <font size= 5> train.py  </font></th>\n",
    "    </tr>\n",
    " </table>\n",
    " <code>\n",
    "imdb = kitti(FLAGS.image_set, FLAGS.data_path, mc)    \n",
    "#imdb = kitti('train', '../data/KITTI', mc)\n",
    "</code>\n",
    "\n",
    "<table style=\"width:100%;border-collapse:collapse;background-color:white;\">\n",
    "    <tr style=\"background-color:rgba(30, 200,100, 0.8)\" >\n",
    "      <th> <font size= 5> imdb.py  </font></th>\n",
    "    </tr>\n",
    " </table>\n",
    "<code>\n",
    "def read_batch(self, shuffle=True):\n",
    "   for idx in batch_idx:\n",
    "      im = cv2.imread(self._image_path_at(idx)).astype(np.float32, copy=False)\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font color=#8552a1 size= 6>\n",
    "  training gt\n",
    "</font>\n",
    "<hr class=\"docutils\" />\n",
    "\n",
    "<table style=\"width:100%;border-collapse:collapse;background-color:white;\">\n",
    "    <tr style=\"background-color:rgba(30, 200,100, 0.8)\" >\n",
    "      <th> <font size= 5> train.py  </font></th>\n",
    "    </tr>\n",
    " </table>\n",
    " <code>\n",
    "tf.app.flags.DEFINE_string('dataset', 'KITTI',\n",
    "                           \"\"\"Currently only support KITTI dataset.\"\"\")\n",
    "tf.app.flags.DEFINE_string('data_path', '', \"\"\"Root directory of data\"\"\")\n",
    "tf.app.flags.DEFINE_string('image_set', 'train',\n",
    "                           \"\"\" Can be train, trainval, val, or test\"\"\")\n",
    "</code>\n",
    "\n",
    "<table style=\"width:100%;border-collapse:collapse;background-color:white;\">\n",
    "    <tr style=\"background-color:rgba(30, 200,100, 0.8)\" >\n",
    "      <th> <font size= 5> kitty.py  </font></th>\n",
    "    </tr>\n",
    " </table>\n",
    "<code>\n",
    "class kitti(imdb):\n",
    "  def __init__(self, image_set, data_path, mc):\n",
    "    imdb.__init__(self, 'kitti_'+image_set, mc)\n",
    "    self._data_root_path = data_path\n",
    "    self._label_path = os.path.join(self._data_root_path, 'training', 'label_2')\n",
    "    self._rois = self._load_kitti_annotation()\n",
    "    def _load_kitti_annotation(self):\n",
    "      for index in self._image_idx:\n",
    "        filename = os.path.join(self._label_path, index+'.txt')\n",
    "  \n",
    "</code>\n",
    "\n",
    "<table style=\"width:100%;border-collapse:collapse;background-color:white;\">\n",
    "    <tr style=\"background-color:rgba(30, 200,100, 0.8)\" >\n",
    "      <th> <font size= 5> imdb.py  </font></th>\n",
    "    </tr>\n",
    " </table>\n",
    "<code>\n",
    "def read_batch(self, shuffle=True):\n",
    "  gt_bbox = np.array([[b[0], b[1], b[2], b[3]] for b in self._rois[idx][:]])\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#8552a1 size= 6>\n",
    "read_image_batch\n",
    "</font>\n",
    "<hr class=\"docutils\" />\n",
    "\n",
    "<table style=\"width:100%;border-collapse:collapse;background-color:white;\">\n",
    "    <tr style=\"background-color:rgba(30, 200,100, 0.8)\" >\n",
    "      <th> <font size= 5> eval.py  </font></th>\n",
    "    </tr>\n",
    " </table>\n",
    "<code>\n",
    "images, scales = imdb.read_image_batch(shuffle=False)\n",
    "</code>\n",
    "\n",
    "<table style=\"width:100%;border-collapse:collapse;background-color:white;\">\n",
    "    <tr style=\"background-color:rgba(30, 200,100, 0.8)\" >\n",
    "      <th> <font size= 5> imdb.py  </font></th>\n",
    "    </tr>\n",
    " </table>\n",
    "<code>\n",
    "def read_image_batch(self, shuffle=True):\n",
    "</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
